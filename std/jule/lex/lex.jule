// Copyright 2023-2024 The Jule Programming Language.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use std::jule::build::{logf, LogMsg, Log, LogKind}
use utf8 for std::unicode::utf8

// Lexer mode.
pub enum LexMode {
    Standard = 0 << 0, // Standard mode.
    Comment = 1 << 0,  // Standard mode + comments.
}

struct KindPair {
    kind: TokenKind
    id:   TokenId
}

static KEYWORDS: [...]KindPair = [
    {TokenKind.I8, TokenId.Prim},
    {TokenKind.I16, TokenId.Prim},
    {TokenKind.I32, TokenId.Prim},
    {TokenKind.I64, TokenId.Prim},
    {TokenKind.U8, TokenId.Prim},
    {TokenKind.U16, TokenId.Prim},
    {TokenKind.U32, TokenId.Prim},
    {TokenKind.U64, TokenId.Prim},
    {TokenKind.F32, TokenId.Prim},
    {TokenKind.F64, TokenId.Prim},
    {TokenKind.Uint, TokenId.Prim},
    {TokenKind.Int, TokenId.Prim},
    {TokenKind.Uintptr, TokenId.Prim},
    {TokenKind.Bool, TokenId.Prim},
    {TokenKind.Str, TokenId.Prim},
    {TokenKind.Any, TokenId.Prim},
    {TokenKind.True, TokenId.Lit},
    {TokenKind.False, TokenId.Lit},
    {TokenKind.Nil, TokenId.Lit},
    {TokenKind.Const, TokenId.Const},
    {TokenKind.Ret, TokenId.Ret},
    {TokenKind.Type, TokenId.Type},
    {TokenKind.For, TokenId.For},
    {TokenKind.Break, TokenId.Break},
    {TokenKind.Cont, TokenId.Cont},
    {TokenKind.In, TokenId.In},
    {TokenKind.If, TokenId.If},
    {TokenKind.Else, TokenId.Else},
    {TokenKind.Use, TokenId.Use},
    {TokenKind.Pub, TokenId.Pub},
    {TokenKind.Goto, TokenId.Goto},
    {TokenKind.Enum, TokenId.Enum},
    {TokenKind.Struct, TokenId.Struct},
    {TokenKind.Co, TokenId.Co},
    {TokenKind.Match, TokenId.Match},
    {TokenKind.Self, TokenId.Self},
    {TokenKind.Trait, TokenId.Trait},
    {TokenKind.Impl, TokenId.Impl},
    {TokenKind.Cpp, TokenId.Cpp},
    {TokenKind.Fall, TokenId.Fall},
    {TokenKind.Fn, TokenId.Fn},
    {TokenKind.Let, TokenId.Let},
    {TokenKind.Unsafe, TokenId.Unsafe},
    {TokenKind.Mut, TokenId.Mut},
    {TokenKind.Defer, TokenId.Defer},
    {TokenKind.Static, TokenId.Static},
    {TokenKind.Error, TokenId.Error},
]

static BASIC_OPS: [...]KindPair = [
    {TokenKind.DblColon, TokenId.DblColon},
    {TokenKind.Colon, TokenId.Colon},
    {TokenKind.Semicolon, TokenId.Semicolon},
    {TokenKind.Comma, TokenId.Comma},
    {TokenKind.TripleDot, TokenId.Op},
    {TokenKind.Dot, TokenId.Dot},
    {TokenKind.PlusEq, TokenId.Op},
    {TokenKind.MinusEq, TokenId.Op},
    {TokenKind.StarEq, TokenId.Op},
    {TokenKind.SolidusEq, TokenId.Op},
    {TokenKind.PercentEq, TokenId.Op},
    {TokenKind.LshiftEq, TokenId.Op},
    {TokenKind.RshiftEq, TokenId.Op},
    {TokenKind.CaretEq, TokenId.Op},
    {TokenKind.AmperEq, TokenId.Op},
    {TokenKind.VlineEq, TokenId.Op},
    {TokenKind.Eqs, TokenId.Op},
    {TokenKind.NotEq, TokenId.Op},
    {TokenKind.GreatEq, TokenId.Op},
    {TokenKind.LessEq, TokenId.Op},
    {TokenKind.DblAmper, TokenId.Op},
    {TokenKind.DblVline, TokenId.Op},
    {TokenKind.Lshift, TokenId.Op},
    {TokenKind.Rshift, TokenId.Op},
    {TokenKind.DblPlus, TokenId.Op},
    {TokenKind.DblMinus, TokenId.Op},
    {TokenKind.Plus, TokenId.Op},
    {TokenKind.Minus, TokenId.Op},
    {TokenKind.Star, TokenId.Op},
    {TokenKind.Solidus, TokenId.Op},
    {TokenKind.Percent, TokenId.Op},
    {TokenKind.Amper, TokenId.Op},
    {TokenKind.Vline, TokenId.Op},
    {TokenKind.Caret, TokenId.Op},
    {TokenKind.Excl, TokenId.Op},
    {TokenKind.Lt, TokenId.Op},
    {TokenKind.Gt, TokenId.Op},
    {TokenKind.Eq, TokenId.Op},
    {TokenKind.Hash, TokenId.Hash},
]

fn make_err(row: int, col: int, &f: &File, fmt: LogMsg, args: ...any): Log {
    ret Log{
        kind: LogKind.Error,
        row: row,
        column: col,
        path: f.path,
        text: logf(fmt, args...),
    }
}

fn bytes_has_prefix(&bytes: []byte, prefix: str): bool {
    if bytes.len < prefix.len {
        ret false
    }
    for i in prefix {
        if bytes[i] != prefix[i] {
            ret false
        }
    }
    ret true
}

fn float_fmt_e(&txt: []byte, mut i: int): (lit: str) {
    i++ // Skip E | e
    if i >= txt.len {
        ret
    }

    let mut b = txt[i]
    if b == '_' {
        ret
    }
    if b == '+' || b == '-' {
        i++ // Skip operator
        if i >= txt.len {
            ret
        }
        if txt[i] == '_' {
            ret
        }
    }

    let first = i
    for i < txt.len; i++ {
        b = txt[i]
        if b != '_' && !is_decimal(b) {
            break
        }
    }

    if i == first {
        ret ""
    }
    ret str(txt[:i])
}

fn float_fmt_p(&txt: []byte, i: int): str {
    ret float_fmt_e(txt, i)
}

fn float_fmt_dotnp(&txt: []byte, mut i: int): str {
    if txt[i] != '.' {
        ret ""
    }

    i++
loop:
    for i < txt.len; i++ {
        let b = txt[i]
        match {
        | b == '_' | is_decimal(b):
            continue
        | is_float_fmt_p(b, i):
            ret float_fmt_p(txt, i)
        |:
            break loop
        }
    }
    ret ""
}

fn float_fmt_dotfp(&txt: []byte, mut i: int): str {
    i += 2 // skip .f
    ret float_fmt_e(txt, i)
}

fn float_fmt_dotp(&txt: []byte, mut i: int): str {
    i++ // skip .
    ret float_fmt_e(txt, i)
}

fn float_num(&txt: []byte, mut i: int): (lit: str) {
    i++ // Skip dot
    if i >= txt.len {
        ret str(txt)
    }
    if txt[i] == '_' {
        i--
        ret str(txt[:i])
    }
    for i < txt.len; i++ {
        let b = txt[i]
        if i > 1 && (b == 'e' || b == 'E') {
            ret float_fmt_e(txt, i)
        }
        if b != '_' && !is_decimal(b) {
            break
        }
    }

    if i == 1 { // Just dot
        ret
    }
    ret str(txt[:i])
}

fn common_num(&txt: []byte): (lit: str) {
    let mut i = 0
loop:
    for i < txt.len; i++ {
        let b = txt[i]
        match {
        | b == '.':
            ret float_num(txt, i)
        | b == '_':
            continue
        | is_float_fmt_e(b, i):
            ret float_fmt_e(txt, i)
        | !is_decimal(b):
            break loop
        }
    }

    if i == 0 {
        ret
    }
    ret str(txt[:i])
}

fn binary_num(&txt: []byte): (lit: str) {
    if !bytes_has_prefix(txt, "0b") {
        ret ""
    }
    if txt.len < 2 {
        ret
    }

    const BINARY_START = 2
    let mut i = BINARY_START
    for i < txt.len; i++ {
        if txt[i] != '_' && !is_binary(txt[i]) {
            break
        }
    }

    if i == BINARY_START {
        ret
    }
    ret str(txt[:i])
}

fn is_float_fmt_e(b: byte, i: int): bool {
    ret i > 0 && (b == 'e' || b == 'E')
}

fn is_float_fmt_p(b: byte, i: int): bool {
    ret i > 0 && (b == 'p' || b == 'P')
}

fn is_float_fmt_dotnp(&txt: []byte, mut i: int): bool {
    if txt[i] != '.' {
        ret false
    }
    i++
loop:
    for i < txt.len; i++ {
        let b = txt[i]
        match {
        | b == '_' | is_decimal(b):
            continue
        | is_float_fmt_p(b, i):
            ret true
        |:
            break loop
        }
    }

    ret false
}

fn is_float_fmt_dotp(&txt: []byte, i: int): bool {
    match {
    | txt.len < 3:
        fall
    | txt[i] != '.':
        fall
    | txt[i+1] != 'p' && txt[i+1] != 'P':
        ret false
    |:
        ret true
    }
}

fn is_float_fmt_dotfp(&txt: []byte, i: int): bool {
    match {
    | txt.len < 4:
        fall
    | txt[i] != '.':
        fall
    | txt[i+1] != 'f' && txt[i+1] != 'F':
        fall
    | txt[i+2] != 'p' && txt[i+1] != 'P':
        ret false
    |:
        ret true
    }
}

fn octal_num(&txt: []byte): (lit: str) {
    if txt[0] != '0' {
        ret ""
    }
    if txt.len < 2 {
        ret
    }

    let mut octal_start = 1

    let mut o = false
    if txt[1] == 'o' {
        if txt.len < 3 {
            ret
        }
        octal_start++
        o = true
    }

    let mut i = octal_start
    for i < txt.len; i++ {
        let b = txt[i]
        if b == '.' {
            if o {
                ret ""
            }
            ret float_num(txt, i)
        }
        if is_float_fmt_e(b, i) {
            ret float_fmt_e(txt, i)
        }
        if b != '_' && !is_octal(b) {
            break
        }
    }

    if i == octal_start {
        ret
    }
    ret str(txt[:i])
}

fn hex_num(&txt: []byte): (lit: str) {
    if txt.len < 3 {
        ret
    }
    if txt[0] != '0' || (txt[1] != 'x' && txt[1] != 'X') {
        ret
    }

    const HEX_START = 2
    let mut i = HEX_START
loop:
    for i < txt.len; i++ {
        let b = txt[i]
        match {
        | is_float_fmt_dotp(txt, i):
            ret float_fmt_dotp(txt, i)
        | is_float_fmt_dotfp(txt, i):
            ret float_fmt_dotfp(txt, i)
        | is_float_fmt_p(b, i):
            ret float_fmt_p(txt, i)
        | is_float_fmt_dotnp(txt, i):
            ret float_fmt_dotnp(txt, i)
        | b != '_' && !is_hex(b):
            break loop
        }
    }

    if i == HEX_START {
        ret
    }
    ret str(txt[:i])
}

fn hex_escape(&txt: []byte, n: int): (seq: str) {
    if txt.len < n {
        ret
    }

    const HEX_START = 2
    let mut i = HEX_START
    for i < n; i++ {
        if !is_hex(txt[i]) {
            ret
        }
    }

    seq = str(txt[:n])
    ret
}

// Pattern (RegEx): ^\\U.{8}
fn big_unicode_point_escape(&txt: []byte): str {
    ret hex_escape(txt, 10)
}

// Pattern (RegEx): ^\\u.{4}
fn little_unicode_point_escape(&txt: []byte): str {
    ret hex_escape(txt, 6)
}

// Pattern (RegEx): ^\\x..
fn hex_byte_escape(&txt: []byte): str {
    ret hex_escape(txt, 4)
}

// Patter (RegEx): ^\\[0-7]{3}
fn byte_escape(&txt: []byte): (seq: str) {
    if txt.len < 4 {
        ret
    }
    if !is_octal(txt[1]) || !is_octal(txt[2]) || !is_octal(txt[3]) {
        ret
    }
    ret str(txt[:4])
}

struct Lex {
    mode:   LexMode
    tokens: []Token
    file:   &File
    pos:    int
    column: int
    row:    int
    errors: []Log
}

impl Lex {
    fn push_err(mut self, fmt: LogMsg, args: ...any) {
        self.errors = append(self.errors,
            make_err(self.row, self.column, self.file, fmt, args...))
    }

    fn push_err_tok(mut self, &token: Token, fmt: LogMsg) {
        self.errors = append(self.errors,
            make_err(token.row, token.column, self.file, fmt))
    }

    // Lexs all source content.
    fn lex(mut self) {
        self.errors = nil
        self.new_line()
        for self.pos < self.file.data.len {
            let mut token = self.token()
            if token.id != TokenId.Na {
                self.tokens = append(self.tokens, token)
            }
        }
    }

    // Returns identifer if next token is identifer,
    // returns empty string if not.
    fn id(mut self, &ln: []byte): str {
        if ln.len == 0 {
            ret ""
        }
        let (r, mut i) = utf8::decode_rune(ln)
        if r != '_' && !is_letter(r) {
            ret ""
        }

        for i < ln.len {
            let (pr, n) = utf8::decode_rune(ln[i:])
            if pr != '_' && !is_decimal(byte(pr)) && !is_letter(pr) {
                self.pos += i
                ret str(ln[:i])
            }
            i += n
        }

        self.pos += ln.len
        ret str(ln)
    }

    // Resume to lex from position.
    fn resume(mut self): []byte {
        // Skip spaces.
        let mut i = self.pos
        for i < self.file.data.len; i++ {
            let r = rune(self.file.data[i])
            if is_space(r) {
                const TAB_LEN = 8
                self.pos++
                match r {
                | '\n':
                    self.new_line()
                | '\t':
                    self.column += TAB_LEN
                |:
                    self.column++
                }
                continue
            }

            let mut j = i
            for j < self.file.data.len; j++ {
                if self.file.data[j] == '\n' {
                    break
                }
            }

            ret self.file.data[i:j]
        }
        ret nil
    }

    fn lex_line_comment(mut self, mut &token: Token) {
        let start = self.pos
        self.pos += 2
        for self.pos < self.file.data.len; self.pos++ {
            let r = self.file.data[self.pos]
            if r == '\n' || r == '\r' {
                break
            }
        }
        if self.mode&LexMode.Comment == LexMode.Comment {
            token.id = TokenId.Comment
            token.kind = str(self.file.data[start:self.pos])
        }
    }

    fn lex_range_comment(mut self, mut &token: Token) {
        let start = self.pos
        self.pos += 2
        for self.pos < self.file.data.len; self.pos++ {
            let r = self.file.data[self.pos]
            if r == '\r' {
                continue
            }
            if r == '\n' {
                self.new_line()
                continue
            }
            self.column += 1
            if self.pos+1 < self.file.data.len && r == '*' &&
                self.file.data[self.pos+1] == '/' {
                self.column += 2
                self.pos += 2
                if self.mode&LexMode.Comment == LexMode.Comment {
                    token.id = TokenId.Comment
                    token.kind = str(self.file.data[start:self.pos])
                }
                ret
            }
        }
        self.push_err(LogMsg.MissingBlockCommentClose)
    }

    // Returns literal if next token is numeric, returns empty string if not.
    fn num(mut self, &txt: []byte): (lit: str) {
        if txt[0] == '_' {
            ret ""
        }
        lit = hex_num(txt)
        if lit != "" {
            goto end
        }
        lit = octal_num(txt)
        if lit != "" {
            goto end
        }
        lit = binary_num(txt)
        if lit != "" {
            goto end
        }
        lit = common_num(txt)
    end:
        self.pos += lit.len
        ret
    }

    fn escape_seq(mut self, &txt: []byte): str {
        let mut seq = ""
        if txt.len < 2 {
            goto end
        }

        match txt[1] {
        | '\\' | '\'' | '"' | 'a' | 'b' | 'f' | 'n' | 'r' | 't' | 'v':
            self.pos += 2
            ret str(txt[:2])
        | 'U':
            seq = big_unicode_point_escape(txt)
        | 'u':
            seq = little_unicode_point_escape(txt)
        | 'x':
            seq = hex_byte_escape(txt)
        |:
            seq = byte_escape(txt)
        }

    end:
        if seq == "" {
            self.pos++
            self.push_err(LogMsg.InvalidEscapeSeq)
            ret ""
        }
        self.pos += seq.len
        ret seq
    }

    fn get_rune(mut self, &txt: []byte, raw: bool): str {
        if !raw && txt[0] == '\\' {
            ret self.escape_seq(txt)
        }

        let (r, n) = utf8::decode_rune(txt)
        self.pos += n
        ret str(r)
    }

    fn lex_rune(mut self, &txt: []byte): str {
        let mut run = "'"
        self.column++
        let mut n = 0
        let mut i = 1
        for i < txt.len; i++ {
            if txt[i] == '\r' {
                continue
            }
            if txt[i] == '\n' {
                self.push_err(LogMsg.MissingRuneEnd)
                self.pos++
                self.new_line()
                ret ""
            }

            let part = txt[i:]
            let r = self.get_rune(part, false)
            run += r
            self.column += utf8::rune_count_str(r)
            if r == "'" {
                self.pos++
                break
            }
            if r.len > 1 {
                i += r.len - 1
            }
            n++
        }

        if n == 0 {
            self.push_err(LogMsg.RuneEmpty)
        } else if n > 1 {
            self.push_err(LogMsg.RuneOverflow)
        }

        ret run
    }

    fn lex_str(mut self): str {
        let mut s = ""
        let mark = self.file.data[self.pos]
        self.pos++ // Skip mark
        let raw = mark == '`'
        s += str(mark)
        self.column++

        for self.pos < self.file.data.len {
            let ch = self.file.data[self.pos]
            if ch == '\r' {
                continue
            }
            if ch == '\n' {
                self.new_line()
                if !raw {
                    self.push_err(LogMsg.MissingStrEnd)
                    self.pos++
                    ret ""
                }
            }
            let mut part = self.file.data[self.pos:]
            let r = self.get_rune(part, raw)
            s += r
            self.column += utf8::rune_count_str(r)
            if ch == mark {
                break
            }
        }

        ret s
    }

    fn is_first_token_of_line(self): bool {
        ret self.column == 1
    }

    fn new_line(mut self) {
        self.row++
        self.column = 1
    }

    fn is_op(mut self, &txt: []byte, kind: str, id: TokenId, mut &t: Token): bool {
        if !bytes_has_prefix(txt, kind) {
            ret false
        }

        t.kind = kind
        t.id = id
        self.pos += kind.len
        ret true
    }

    fn lex_basic_ops(mut self, txt: []byte, mut &tok: Token): bool {
        for _, pair in BASIC_OPS {
            if self.is_op(txt, pair.kind, pair.id, tok) {
                ret true
            }
        }

        ret false
    }

    fn lex_id(mut self, &txt: []byte, mut &t: Token): bool {
        let lex = self.id(txt)
        if lex == "" {
            ret false
        }

        t.kind = lex
        t.id = TokenId.Ident
        ret true
    }

    fn lex_num(mut self, &txt: []byte, mut &t: Token): bool {
        let lex = self.num(txt)
        if lex == "" {
            ret false
        }

        t.kind = lex
        t.id = TokenId.Lit
        ret true
    }

    // lex.token generates next token from resume at position.
    fn token(mut self): Token {
        let mut t = Token{file: self.file, id: TokenId.Na}

        let txt = self.resume()
        if txt == nil {
            ret t
        }

        // Set token values.
        t.column = self.column
        t.row = self.row

        //* lex.Tokenenize
        match {
        | self.lex_num(txt, t):
            // Pass.
            break
        | txt[0] == '\'':
            t.kind = self.lex_rune(txt)
            t.id = TokenId.Lit
            ret t
        | txt[0] == '"' || txt[0] == '`':
            t.kind = self.lex_str()
            t.id = TokenId.Lit
            ret t
        | bytes_has_prefix(txt, TokenKind.LnComment):
            self.lex_line_comment(t)
            ret t
        | bytes_has_prefix(txt, TokenKind.RangLComment):
            self.lex_range_comment(t)
            ret t
        | self.is_op(txt, TokenKind.LParent, TokenId.Range, t)
        | self.is_op(txt, TokenKind.RParent, TokenId.Range, t)
        | self.is_op(txt, TokenKind.LBrace, TokenId.Range, t)
        | self.is_op(txt, TokenKind.RBrace, TokenId.Range, t)
        | self.is_op(txt, TokenKind.LBracket, TokenId.Range, t)
        | self.is_op(txt, TokenKind.RBracket, TokenId.Range, t)
        | self.lex_basic_ops(txt, t):
            // Pass.
            break
        | self.lex_id(txt, t):
            for _, pair in KEYWORDS {
                if pair.kind == t.kind {
                    t.id = pair.id
                    break
                }
            }
        |:
            let (r, sz) = utf8::decode_rune(txt)
            self.push_err(LogMsg.InvalidToken, r)
            self.column++
            self.pos += sz
            ret t
        }

        self.column += utf8::rune_count_str(t.kind)
        ret t
    }
}

// Lex source code into fileset.
// Returns nil if f == nil.
// Returns nil slice for errors if no any error.
pub fn lex(mut f: &File, mode: LexMode): []Log {
    if f == nil {
        ret nil
    }

    let mut lex = Lex{
        mode: mode,
        file: f,
        pos: 0,
        row: -1, // For true row
    }

    lex.new_line()
    lex.lex()

    if lex.errors.len > 0 {
        ret lex.errors
    }

    f.tokens = lex.tokens
    ret nil
}